{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4382e6f3",
   "metadata": {},
   "source": [
    "Tarefa 2\n",
    "\n",
    "1. Cite 5 diferenças entre o AdaBoost (Adaptive Boosting) e o GBM (Gradient Boosting Machine).\n",
    "2. Acesse o link Scikit-learn - GBM, leia a explicação e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM.\n",
    "3. Cite 5 Hyperparametros importantes no GBM.\n",
    "4. Acessando o artigo do Jerome Friedman (stochastic) e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039bfa1d",
   "metadata": {},
   "source": [
    "Resposta 1:\n",
    "\n",
    "a) Diferença nas bases:\n",
    "- AdaBoost: cria modelos fracos com Stumps e dá maior peso (dá mais importância) para as respostas erradas.\n",
    "Assim, o modelo foca no que ainda não tem uma boa resposta.\n",
    "- GBM: cria modelos de base fracas e utilliza a técnica de impulsionar gradientes, para ajustar o modelo e diminuir a função de perda perante a reposta verdadeira.\n",
    "\n",
    "b) Peso dos modelos:\n",
    "- AdaBoost: ele altera os pesos durante o treinamento, dando maior valor (peso mais alto) para os modelos que tiverem uma resposta errada (falsa).\n",
    "- GBM: não altera os pesos. Ele ajusta modelos base para minimizar o gradiente da função de perda perante a resposta correta (verdadeira).\n",
    "\n",
    "c) Dependência entre os modelos base:\n",
    "- AdaBoost: os modelos base são ajustados em sequencia, o modelo é treinado com base nos erros do modelo anterior.\n",
    "- GBM: os modelos base também são feitos em sequencia, mas, o modelo é treinado para minimizar o gradiente da função de perda em relação a resposta positiva (verdadeira). O GBM não leva em consideração o modelo anterior.\n",
    "\n",
    "d) Sensibilidade:\n",
    "- AdaBoost: é bem sensível a ruídos e outliers, pois, ele dá mais peso para as respostas erradas (negativas).\n",
    "- GBM: não é tão afetado pelos ruídos e outliers, pois, ele utiliza o gradiente da função de perda para ajustar os modelos.\n",
    "\n",
    "e) Função de perda:\n",
    "- AdaBoost: usa geralmente uma função exponencial ou função logística.\n",
    "- GBM: usa geralmente a função de perda de desvio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc91800",
   "metadata": {},
   "source": [
    "Resposta 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c6d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1144d129",
   "metadata": {},
   "source": [
    "Exemplo de classificação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c417801",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_breast_cancer()\n",
    "X, y = df.data, df.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcacea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplo_classificacao = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0578e8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(random_state=42)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exemplo_classificacao.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da7f0e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predito = exemplo_classificacao.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99fcdbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5231004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "acuracia = accuracy_score(y_test, y_predito)\n",
    "print(\"Acurácia:\", acuracia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89b14a",
   "metadata": {},
   "source": [
    "Exemplo de regressão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4faf64cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0de8578",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_boston()\n",
    "X, y = df.data, df.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18fd7d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplo_regressao = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "726f6b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(random_state=42)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exemplo_regressao.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39206a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predito = exemplo_regressao.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b467669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.4497613 , 31.46136029, 17.70531345, 24.02257316, 17.6811439 ,\n",
       "       22.03112796, 18.14958762, 13.83045241, 20.61619342, 21.04285745,\n",
       "       19.93787694, 17.84771967,  9.00542602, 21.75820976, 19.79454516,\n",
       "       25.71887535, 20.04177142,  8.83726437, 45.07118382, 15.96280089,\n",
       "       24.1976327 , 25.26461031, 12.51887607, 20.89492763, 14.97826629,\n",
       "       15.47812803, 22.20367497, 13.26656609, 19.27766026, 21.43650957,\n",
       "       19.72895052, 23.47588201, 19.38092692, 19.15659237, 14.50437732,\n",
       "       16.94131142, 33.09630021, 19.80432738, 20.7291071 , 24.14928838,\n",
       "       18.30166138, 30.36297218, 44.94906542, 20.84894217, 22.61816613,\n",
       "       14.61317262, 15.71182991, 24.14928838, 18.06929082, 28.08800612,\n",
       "       20.23520559, 35.66832821, 16.73377496, 24.99108092, 47.7594972 ,\n",
       "       21.39870322, 16.51753536, 32.24969813, 22.03440415, 18.08425056,\n",
       "       24.24590305, 34.58456084, 30.66211615, 19.1946555 , 24.54464101,\n",
       "       16.92131694, 14.50851517, 23.66112578, 28.04012929, 15.40476214,\n",
       "       21.27417609, 24.96901377, 11.04430788, 21.36306687, 23.04411537,\n",
       "        6.34609352, 20.27445864, 45.93020001, 11.79854943, 12.89228027,\n",
       "       21.61669023, 13.32297788, 19.93114558, 10.55419493, 20.34218788,\n",
       "       26.96934638, 15.44188279, 23.6166354 , 25.62099461, 17.54812759,\n",
       "       22.36589817, 10.00067281, 19.28428371, 19.08093255, 22.71668007,\n",
       "       19.44786097, 40.43041915, 12.08828711, 12.01055212, 14.5398469 ,\n",
       "       20.67949841, 22.88578766])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cac873a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro médio quadrado: 6.208861361528038\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test, y_predito)\n",
    "print(\"Erro médio quadrado:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29d445e",
   "metadata": {},
   "source": [
    "Resposta 3:\n",
    "\n",
    "a) n_estimators - esse hiperparâmetro mostra quantos modelos base (estimadores) vão ser usados.<br>\n",
    "b) learning_rate (taxa de aprendizado) - mostra o quanto cada modelo base ajuda no conjunto final de modelos, qual o peso de cada um.<br>\n",
    "c) max_depth - é a profundidade máxima da árvore. Quanto maior esse valor, maior vai ser a árvore.<br>\n",
    "d) subsample - é a quantidade de amostras usadas em cada árvore.<br>\n",
    "e) reg_alpha - ele controla a perda durante o treinamento do GBM. Um maior reg_alpha ajuda a evitar overfitting.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a15be4",
   "metadata": {},
   "source": [
    "Resposta 4:\n",
    "\n",
    "O GBM como dito anteriormente, cria modelos base em sequencia para diminuir o gradiente da função de perda.<br>\n",
    "Já o SGBM (Stochastic Gradient Boosting Machine) utiliza amostragem aleatória dos dados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
